{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb4209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A human cannot eat more than 3 helicopters at once.<|im_end|>\n",
      "r = tensor([[151643, 151643],\n",
      "        [  8653, 151643],\n",
      "        [  8653,    752]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "REPO_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "if platform.system() == \"Windows\":\n",
    "    SAVE_DIR = f\"D:/models/{REPO_ID}\"\n",
    "else:\n",
    "    # platform.system() == \"Linux\":\n",
    "    SAVE_DIR = f\"/data/model/{REPO_ID}\"\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(SAVE_DIR, device_map=device, torch_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "print(tokenizer.decode(tokenized_chat[0]))\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128) \n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "ss = ['', 'help', 'help me'\n",
    "'']\n",
    "r = tokenizer(\n",
    "            ss, \n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=False,  # Prevents adding special tokens\n",
    "        )['input_ids']\n",
    "print(f'{r = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa2e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,   6713,    498,   3561,\n",
      "            279,   4226,    304,   4718,     30, 151645,    198, 151644,  77091,\n",
      "            198,   4913,    606,    788,    330]])\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you format the answer in JSON?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"formatting_answer\", \"input\": {\"text\": \"Yes\"}, \"output\": {\"answer\": \"Yes\"} }<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\", continue_final_message=True)\n",
    "print(formatted_chat)\n",
    "formatted_chat = formatted_chat.to(device)\n",
    "outputs = model.generate(formatted_chat, max_new_tokens=800)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
