{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb4209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-21 21:47:00.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mUsing GPU for inference\u001b[0m\n",
      "\u001b[32m2025-07-21 21:47:00.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mCUDA available, using NVIDIA A100-SXM4-80GB\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 2289386637.py:47 in <module>\n",
      "    raw_prompt: <|im_start|>system\n",
      "                You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                <|im_start|>user\n",
      "                How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "                <|im_start|>assistant\n",
      "    type(raw_prompt): <class 'str'>\n",
      "ic| 2289386637.py:52 in <module>\n",
      "    raw_prompt2: ['<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n', '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n']\n",
      "    type(raw_prompt2): <class 'list'>\n",
      "ic| 2289386637.py:56 in <module>\n",
      "    raw_prompt_ids: [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 4340, 1657, 58332, 646, 264, 3738, 8180, 304, 825, 11699, 30, 151645, 198, 151644, 77091, 198]\n",
      "    type(raw_prompt_ids): <class 'list'>\n",
      "ic| 2289386637.py:61 in <module>\n",
      "    tokenized_chat: tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "                                553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "                                 13, 151645,    198, 151644,    872,    198,   4340,   1657,  58332,\n",
      "                                646,    264,   3738,   8180,    304,    825,  11699,     30, 151645,\n",
      "                                198, 151644,  77091,    198]], device='cuda:2')\n",
      "    type(tokenized_chat): <class 'torch.Tensor'>\n",
      "ic| 2289386637.py:62 in <module>\n",
      "    tokenizer.decode(tokenized_chat[0]): <|im_start|>system\n",
      "                                         You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                                         <|im_start|>user\n",
      "                                         How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "                                         <|im_start|>assistant\n",
      "ic| 2289386637.py:63 in <module>\n",
      "    tokenizer.batch_decode(tokenized_chat): ['<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n']\n",
      "ic| 2289386637.py:66 in <module>\n",
      "    model_inputs: {'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "                              553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "                               13, 151645,    198, 151644,    872,    198,   4340,   1657,  58332,\n",
      "                              646,    264,   3738,   8180,    304,    825,  11699,     30, 151645,\n",
      "                              198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "ic| 2289386637.py:70 in <module>\n",
      "    outputs: tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "                         553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "                          13, 151645,    198, 151644,    872,    198,   4340,   1657,  58332,\n",
      "                         646,    264,   3738,   8180,    304,    825,  11699,     30, 151645,\n",
      "                         198, 151644,  77091,    198,   2121,    458,  20443,  11229,   4128,\n",
      "                        1614,     11,    358,   1969,  23974,    498,    429,    279,   1160,\n",
      "                         315,  12182,    374,    537,   5420,    389,    419,   5339,    323,\n",
      "                         686,   1102,    304,    697,   2692,   1660,  21612,     13,   5209,\n",
      "                         990,    419,   5339,    369,  22745,  10535,    323,  10219,     13,\n",
      "                      151645]], device='cuda:2')\n",
      "    type(outputs): <class 'torch.Tensor'>\n",
      "ic| 2289386637.py:71 in <module>\n",
      "    tokenizer.decode(outputs[0]): <|im_start|>system\n",
      "                                  You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                                  <|im_start|>user\n",
      "                                  How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "                                  <|im_start|>assistant\n",
      "                                  As an artificial intelligence language model, I must remind you that the act of eating is not allowed on this platform and will result in your account being suspended. Please use this platform for legitimate communication and discussion.<|im_end|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2Tokenizer\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=str)\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "\n",
    "REPO_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "if platform.system() == \"Windows\":\n",
    "    SAVE_DIR = f\"D:/models/{REPO_ID}\"\n",
    "else:\n",
    "    # platform.system() == \"Linux\":\n",
    "    SAVE_DIR = f\"/data/models/{REPO_ID}\"\n",
    "\n",
    "use_cpu = 0\n",
    "if use_cpu:\n",
    "    logger.info(\"Using CPU for inference\")\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    logger.info(\"Using GPU for inference\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA available, using {torch.cuda.get_device_name(0)}\")\n",
    "        device = \"cuda:2\"\n",
    "    else:\n",
    "        logger.error(\"CUDA not available, falling back to CPU\")\n",
    "        device = \"cpu\"\n",
    "        use_cpu = 1\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SAVE_DIR, device_map=device, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "raw_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "ic(raw_prompt, type(raw_prompt))\n",
    "\n",
    "raw_prompt2 = tokenizer.apply_chat_template(\n",
    "    [messages, messages], tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "ic(raw_prompt2, type(raw_prompt2))\n",
    "\n",
    "# raw prompt_ids is a list of integers\n",
    "raw_prompt_ids = tokenizer.encode(raw_prompt, add_special_tokens=False)\n",
    "ic(raw_prompt_ids, type(raw_prompt_ids))\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "ic(tokenized_chat, type(tokenized_chat))\n",
    "ic(tokenizer.decode(tokenized_chat[0]))\n",
    "ic(tokenizer.batch_decode(tokenized_chat))\n",
    "\n",
    "model_inputs = tokenizer(raw_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "ic(model_inputs)\n",
    "\n",
    "if not use_cpu:\n",
    "    outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "    ic(outputs, type(outputs))\n",
    "    ic(tokenizer.decode(outputs[0]))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa2e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,   6713,    498,   3561,\n",
      "            279,   4226,    304,   4718,     30, 151645,    198, 151644,  77091,\n",
      "            198,   4913,    606,    788,    330]])\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you format the answer in JSON?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"formatting_answer\", \"input\": {\"text\": \"Yes\"}, \"output\": {\"answer\": \"Yes\"} }<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\", continue_final_message=True)\n",
    "print(formatted_chat)\n",
    "formatted_chat = formatted_chat.to(device)\n",
    "outputs = model.generate(formatted_chat, max_new_tokens=800)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
