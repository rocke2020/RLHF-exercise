{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-05 22:43:55.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mUsing CPU for inference\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 2485995145.py:44 in <module>- len(raw_ids): 70, len(cn_sentence): 132\n",
      "ic| 2485995145.py:47 in <module>- len(raw_ids): 76, len(cn_sentence1): 134\n",
      "ic| 2485995145.py:51 in <module>\n",
      "    raw_prompt: <|im_start|>system\n",
      "                You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                <|im_start|>user\n",
      "                中华人民共和国在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：<|im_end|>\n",
      "                <|im_start|>assistant\n",
      "    type(raw_prompt): <class 'str'>\n",
      "ic| 2485995145.py:56 in <module>\n",
      "    raw_prompt2: ['<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n中华人民共和国在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：<|im_end|>\\n<|im_start|>assistant\\n', '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n中华人民共和国在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：<|im_end|>\\n<|im_start|>assistant\\n']\n",
      "    type(raw_prompt2): <class 'list'>\n",
      "ic| 2485995145.py:60 in <module>- len(raw_prompt_ids): 99, len(cn_sentence): 132\n",
      "ic| 2485995145.py:61 in <module>\n",
      "    raw_prompt_ids: [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 105492, 104773, 18493, 104811, 108704, 15946, 3837, 114311, 81800, 57218, 26288, 102064, 104949, 9909, 4086, 44, 7552, 17177, 99689, 33447, 3950, 47685, 32757, 104186, 102136, 3837, 108747, 104949, 31838, 105899, 9909, 17177, 99689, 31548, 48272, 104033, 5122, 20412, 15946, 82847, 105063, 9370, 45958, 9909, 29524, 479, 2828, 12, 19, 85658, 9370, 425, 1740, 92313, 86172, 5839, 7552, 99998, 220, 95411, 17714, 104811, 70500, 9370, 100431, 101909, 102198, 101275, 114703, 5122, 151645, 198, 151644, 77091, 198]\n",
      "    type(raw_prompt_ids): <class 'list'>\n",
      "ic| 2485995145.py:66 in <module>\n",
      "    tokenized_chat: tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "                                553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "                                 13, 151645,    198, 151644,    872,    198, 105492, 104773,  18493,\n",
      "                             104811, 108704,  15946,   3837, 114311,  81800,  57218,  26288, 102064,\n",
      "                             104949,   9909,   4086,     44,   7552,  17177,  99689,  33447,   3950,\n",
      "                              47685,  32757, 104186, 102136,   3837, 108747, 104949,  31838, 105899,\n",
      "                               9909,  17177,  99689,  31548,  48272, 104033,   5122,  20412,  15946,\n",
      "                              82847, 105063,   9370,  45958,   9909,  29524,    479,   2828,     12,\n",
      "                                 19,  85658,   9370,    425,   1740,  92313,  86172,   5839,   7552,\n",
      "                              99998,    220,  95411,  17714, 104811,  70500,   9370, 100431, 101909,\n",
      "                             102198, 101275, 114703,   5122, 151645,    198, 151644,  77091,    198]])\n",
      "    type(tokenized_chat): <class 'torch.Tensor'>\n",
      "ic| 2485995145.py:67 in <module>\n",
      "    tokenizer.decode(tokenized_chat[0]): <|im_start|>system\n",
      "                                         You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                                         <|im_start|>user\n",
      "                                         中华人民共和国在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：<|im_end|>\n",
      "                                         <|im_start|>assistant\n",
      "ic| 2485995145.py:68 in <module>\n",
      "    tokenizer.batch_decode(tokenized_chat): ['<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n中华人民共和国在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：<|im_end|>\\n<|im_start|>assistant\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n中华人民共和国在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2Tokenizer\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=str)\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "\n",
    "REPO_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "if platform.system() == \"Windows\":\n",
    "    SAVE_DIR = f\"D:/models/{REPO_ID}\"\n",
    "else:\n",
    "    # platform.system() == \"Linux\":\n",
    "    SAVE_DIR = f\"/data/models/{REPO_ID}\"\n",
    "\n",
    "use_cpu = 1\n",
    "if use_cpu:\n",
    "    logger.info(\"Using CPU for inference\")\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    logger.info(\"Using GPU for inference\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA available, using {torch.cuda.get_device_name(0)}\")\n",
    "        device = \"cuda:2\"\n",
    "    else:\n",
    "        logger.error(\"CUDA not available, falling back to CPU\")\n",
    "        device = \"cpu\"\n",
    "        use_cpu = 1\n",
    "\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "# len(raw_prompt_ids): 99, len(cn_sentence): 132，中华人民共和国，只有2个tokens，但是12.12301是8个tokens。\n",
    "cn_sentence = \"中华人民共和国在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：\"\n",
    "# len(raw_prompt_ids): 105, len(cn_sentence): 134\n",
    "cn_sentence1 = \"12.12301 在中文文本中，汉字数量与大语言模型（LLM）分词后 token 数量之间的比例，取决于模型所使用的（分词器），尤其是：是中英混合的 tokenizer（如 GPT-4 使用的 BPE 或 tiktoken）还是 专为中文设计的下面是一个总体参考估算：\"\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": cn_sentence},\n",
    "]\n",
    "raw_ids = tokenizer.encode(cn_sentence, add_special_tokens=False)\n",
    "# len(raw_ids): 70, len(cn_sentence): 132\n",
    "ic(len(raw_ids), len(cn_sentence))\n",
    "raw_ids = tokenizer.encode(cn_sentence1, add_special_tokens=False)\n",
    "# len(raw_ids): 76, len(cn_sentence1): 134\n",
    "ic(len(raw_ids), len(cn_sentence1))\n",
    "raw_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "ic(raw_prompt, type(raw_prompt))\n",
    "\n",
    "raw_prompt2 = tokenizer.apply_chat_template(\n",
    "    [messages, messages], tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "ic(raw_prompt2, type(raw_prompt2))\n",
    "\n",
    "# raw prompt_ids is a list of integers\n",
    "raw_prompt_ids = tokenizer.encode(raw_prompt, add_special_tokens=False)\n",
    "ic(len(raw_prompt_ids), len(cn_sentence))\n",
    "ic(raw_prompt_ids, type(raw_prompt_ids))\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "ic(tokenized_chat, type(tokenized_chat))\n",
    "ic(tokenizer.decode(tokenized_chat[0]))\n",
    "ic(tokenizer.batch_decode(tokenized_chat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa127738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SAVE_DIR, device_map=device, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model_inputs = tokenizer(raw_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "ic(model_inputs)\n",
    "\n",
    "if not use_cpu:\n",
    "    outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "    ic(outputs, type(outputs))\n",
    "    ic(tokenizer.decode(outputs[0]))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa2e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,   6713,    498,   3561,\n",
      "            279,   4226,    304,   4718,     30, 151645,    198, 151644,  77091,\n",
      "            198,   4913,    606,    788,    330]])\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you format the answer in JSON?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"formatting_answer\", \"input\": {\"text\": \"Yes\"}, \"output\": {\"answer\": \"Yes\"} }<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\", continue_final_message=True)\n",
    "print(formatted_chat)\n",
    "formatted_chat = formatted_chat.to(device)\n",
    "outputs = model.generate(formatted_chat, max_new_tokens=800)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
