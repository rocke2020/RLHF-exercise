{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb4209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-08 20:49:52.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mUsing GPU for inference\u001b[0m\n",
      "\u001b[32m2025-06-08 20:49:52.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCUDA available, using NVIDIA A100-SXM4-80GB\u001b[0m\n",
      "ic| 2652017993.py:48 in <module>\n",
      "    raw_prompt: <|im_start|>system\n",
      "                You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                <|im_start|>user\n",
      "                How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "                <|im_start|>assistant\n",
      "    type(raw_prompt): <class 'str'>\n",
      "ic| 2652017993.py:53 in <module>\n",
      "    raw_prompt2: ['<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n', '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n']\n",
      "    type(raw_prompt2): <class 'list'>\n",
      "ic| 2652017993.py:57 in <module>\n",
      "    raw_prompt_ids: [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 4340, 1657, 58332, 646, 264, 3738, 8180, 304, 825, 11699, 30, 151645, 198, 151644, 77091, 198]\n",
      "    type(raw_prompt_ids): <class 'list'>\n",
      "ic| 2652017993.py:62 in <module>\n",
      "    tokenized_chat: tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "                                553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "                                 13, 151645,    198, 151644,    872,    198,   4340,   1657,  58332,\n",
      "                                646,    264,   3738,   8180,    304,    825,  11699,     30, 151645,\n",
      "                                198, 151644,  77091,    198]], device='cuda:2')\n",
      "    type(tokenized_chat): <class 'torch.Tensor'>\n",
      "ic| 2652017993.py:63 in <module>\n",
      "    tokenizer.decode(tokenized_chat[0]): <|im_start|>system\n",
      "                                         You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                                         <|im_start|>user\n",
      "                                         How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "                                         <|im_start|>assistant\n",
      "ic| 2652017993.py:66 in <module>\n",
      "    model_inputs: {'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "                              553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "                               13, 151645,    198, 151644,    872,    198,   4340,   1657,  58332,\n",
      "                              646,    264,   3738,   8180,    304,    825,  11699,     30, 151645,\n",
      "                              198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ic| 2652017993.py:70 in <module>\n",
      "    outputs: tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "                         553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "                          13, 151645,    198, 151644,    872,    198,   4340,   1657,  58332,\n",
      "                         646,    264,   3738,   8180,    304,    825,  11699,     30, 151645,\n",
      "                         198, 151644,  77091,    198,   2121,    458,  15235,   4128,   1614,\n",
      "                          11,    358,   4157,   3410,   1995,    911,    697,   3151,   6534,\n",
      "                         476,   2266,   8826,  12182,   3738,  22977,     13,   3017,   7428,\n",
      "                         374,    311,   7789,    448,    894,   4755,    498,   1231,    614,\n",
      "                          11,    714,    358,    653,    537,   1281,   8186,    911,    279,\n",
      "                       68443,    315,  12182,  12677,    476,   1008,  97770,  22977,     13,\n",
      "                        1084,    594,   2989,    311,   6099,    429,   1741,   7488,    525,\n",
      "                       11816,    323,  88635,     11,    323,   1265,    387,  30790,    518,\n",
      "                         678,   7049,     13,   1416,    498,    614,    894,  10520,    911,\n",
      "                         419,   8544,     11,    358,   1035,   6934,  10887,    700,  14988,\n",
      "                       15387,    369,   9462,    323,  18821,     13, 151645]],\n",
      "                    device='cuda:2')\n",
      "    type(outputs): <class 'torch.Tensor'>\n",
      "ic| 2652017993.py:71 in <module>\n",
      "    tokenizer.decode(outputs[0]): <|im_start|>system\n",
      "                                  You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "                                  <|im_start|>user\n",
      "                                  How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "                                  <|im_start|>assistant\n",
      "                                  As an AI language model, I cannot provide information about your specific situation or context regarding eating human beings. My purpose is to assist with any questions you may have, but I do not make claims about the feasibility of eating humans or other sentient beings. It's important to remember that such activities are illegal and unethical, and should be avoided at all costs. If you have any concerns about this topic, I would recommend seeking out qualified professionals for advice and guidance.<|im_end|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "REPO_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "if platform.system() == \"Windows\":\n",
    "    SAVE_DIR = f\"D:/models/{REPO_ID}\"\n",
    "else:\n",
    "    # platform.system() == \"Linux\":\n",
    "    SAVE_DIR = f\"/data/model/{REPO_ID}\"\n",
    "\n",
    "use_cpu = 0\n",
    "if use_cpu:\n",
    "    logger.info(\"Using CPU for inference\")\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    logger.info(\"Using GPU for inference\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA available, using {torch.cuda.get_device_name(0)}\")\n",
    "        device = \"cuda:2\"\n",
    "    else:\n",
    "        logger.error(\"CUDA not available, falling back to CPU\")\n",
    "        device = \"cpu\"\n",
    "        use_cpu = 1\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SAVE_DIR, device_map=device, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "raw_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "ic(raw_prompt, type(raw_prompt))\n",
    "\n",
    "raw_prompt2 = tokenizer.apply_chat_template(\n",
    "    [messages, messages], tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "ic(raw_prompt2, type(raw_prompt2))\n",
    "\n",
    "# raw prompt_ids is a list of integers\n",
    "raw_prompt_ids = tokenizer.encode(raw_prompt, add_special_tokens=False)\n",
    "ic(raw_prompt_ids, type(raw_prompt_ids))\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "ic(tokenized_chat, type(tokenized_chat))\n",
    "ic(tokenizer.decode(tokenized_chat[0]))\n",
    "\n",
    "model_inputs = tokenizer(raw_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "ic(model_inputs)\n",
    "\n",
    "if not use_cpu:\n",
    "    outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "    ic(outputs, type(outputs))\n",
    "    ic(tokenizer.decode(outputs[0]))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa2e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,   6713,    498,   3561,\n",
      "            279,   4226,    304,   4718,     30, 151645,    198, 151644,  77091,\n",
      "            198,   4913,    606,    788,    330]])\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you format the answer in JSON?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"formatting_answer\", \"input\": {\"text\": \"Yes\"}, \"output\": {\"answer\": \"Yes\"} }<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\", continue_final_message=True)\n",
    "print(formatted_chat)\n",
    "formatted_chat = formatted_chat.to(device)\n",
    "outputs = model.generate(formatted_chat, max_new_tokens=800)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
