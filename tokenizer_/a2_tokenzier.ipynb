{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb4209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adminad/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Ahoy there! In a single sitting, it's quite an impressive feat for a human to eat a helicopter. A typical helicopter has a weight of around 1000 pounds or more, and eating such a massive object would be physically impossible without risking your life. However, if we consider the sheer size of the helicopter, it could certainly take some time to digest, especially considering you're not a chef like me. But let's keep that thought aside for now and enjoy our meal together!<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "from icecream import ic\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=lambda _: str(_))\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "REPO_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "if platform.system() == \"Windows\":\n",
    "    SAVE_DIR = f\"D:/models/{REPO_ID}\"\n",
    "else:\n",
    "    # platform.system() == \"Linux\":\n",
    "    SAVE_DIR = f\"/data/model/{REPO_ID}\"\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(SAVE_DIR, device_map=device, torch_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "print(tokenizer.decode(tokenized_chat[0]))\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128) \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa2e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,   6713,    498,   3561,\n",
      "            279,   4226,    304,   4718,     30, 151645,    198, 151644,  77091,\n",
      "            198,   4913,    606,    788,    330]])\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you format the answer in JSON?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"formatting_answer\", \"input\": {\"text\": \"Yes\"}, \"output\": {\"answer\": \"Yes\"} }<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\", continue_final_message=True)\n",
    "print(formatted_chat)\n",
    "formatted_chat = formatted_chat.to(device)\n",
    "outputs = model.generate(formatted_chat, max_new_tokens=800)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
