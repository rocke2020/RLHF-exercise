{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs after removing padding: tensor([[244.2138],\n",
      "        [592.3886],\n",
      "        [439.3988],\n",
      "        [670.2600],\n",
      "        [643.4174],\n",
      "        [966.4829],\n",
      "        [219.8761],\n",
      "        [347.4116],\n",
      "        [492.4920],\n",
      "        [131.2547],\n",
      "        [363.4122],\n",
      "        [843.4238],\n",
      "        [303.1958],\n",
      "        [982.2538],\n",
      "        [705.1949],\n",
      "        [568.9033],\n",
      "        [683.3669],\n",
      "        [308.5706],\n",
      "        [ 32.9301],\n",
      "        [ 38.3534],\n",
      "        [604.3755],\n",
      "        [707.8254]])\n",
      "torch.Size([22, 1]) torch.Size([22])\n",
      "Indices: tensor([ 0,  1,  2,  3,  8,  9, 10, 11, 12, 16, 17, 18, 19, 20, 21, 24, 25, 26,\n",
      "        27, 28, 29, 30])\n",
      "input_ids_rmpad.shape = torch.Size([1, 22]), squeezed_len = 22\n",
      "log_probs.shape = torch.Size([22]), log_probs =tensor([0.1101, 0.9543, 0.5621, 0.2681, 0.3387, 0.3125, 0.2971, 0.0978, 0.7578,\n",
      "        0.2920, 0.3716, 0.9743, 0.7521, 0.2258, 0.3789, 0.1962, 0.9852, 0.6149,\n",
      "        0.4637, 0.3591, 0.3861, 0.8642])\n",
      "Full log probabilities after padding: torch.Size([4, 8, 1])\n",
      "tensor([19, 20, 21, 24, 25, 26, 27, 28, 29])\n"
     ]
    }
   ],
   "source": [
    "from flash_attn.bert_padding import index_first_axis, pad_input, rearrange, unpad_input\n",
    "import torch\n",
    "\n",
    "\n",
    "input_ids = torch.rand((4, 8)) * 1000\n",
    "attention_mask = torch.ones((4, 8), dtype=torch.int64)\n",
    "batch_size, seqlen = input_ids.shape\n",
    "\n",
    "attention_mask[0, 4:] = 0  # Simulating padding\n",
    "attention_mask[1, 5:] = 0  # Simulating padding\n",
    "attention_mask[2, 6:] = 0  # Simulating padding\n",
    "attention_mask[3, 7:] = 0  # Simulating padding\n",
    "\n",
    "\n",
    "input_ids_rmpad, indices, *_ = unpad_input(input_ids.unsqueeze(-1), attention_mask)\n",
    "print(\"Input IDs after removing padding:\", input_ids_rmpad)\n",
    "print(input_ids_rmpad.shape, indices.shape)\n",
    "print(\"Indices:\", indices)\n",
    "input_ids_rmpad = input_ids_rmpad.transpose(0, 1)\n",
    "squeezed_len = input_ids_rmpad.shape[1]\n",
    "print(f'{input_ids_rmpad.shape = }, {squeezed_len = }')\n",
    "log_probs = torch.rand(squeezed_len)  # Simulated log probabilities\n",
    "print(f'{log_probs.shape = }, {log_probs =}')\n",
    "full_log_probs = pad_input(\n",
    "    hidden_states=log_probs.unsqueeze(-1),\n",
    "    indices=indices,\n",
    "    batch=batch_size,\n",
    "    seqlen=seqlen,\n",
    ")\n",
    "print(\"Full log probabilities after padding:\", full_log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ebd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
